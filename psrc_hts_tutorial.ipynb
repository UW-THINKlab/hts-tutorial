{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puget Sound Regional Council Household Travel Survey (PSRC HTS) Tutorial\n",
    "\n",
    "PSRC is the metroplitan planning organization (MPO) that spans King, Kitsap, Pierce and Snohomish County in Washington State. Every few years, they conduct a regional household travel survey collecting both socioedmographic and travel behavior data of a small sample of individuals in the region.\n",
    "\n",
    "Please obtain the PSRC HTS data from their website for the persons, household, and trips level. Additional information is available for further reading.\n",
    "https://household-travel-survey-psregcncl.hub.arcgis.com/\n",
    "\n",
    "Surveys are collected either via a smartphone app or online. Additional trace data is generated from the smartphone survey (only available 2017/2019). Online survey data generates rough O/D lat/lon information (available for 2017, 2019, and 2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trips:** trip-level information, such as depart/arrival time, origin/dest purpose, O/D census tracts, length of trip, speed, mode, etc.\n",
    "\n",
    "**Persons:** person-level information, such as age, race, gender, employment status, industry, etc. \n",
    "\n",
    "**Households:** household-level information, such as size, number of children, lifecycle, location of home, attiudinal characteristics, etc.\n",
    "\n",
    "See the codebook (at above link) for all available variables and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "from pyproj import CRS\n",
    "import geopy.distance\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "# download these fro PSRC's website\n",
    "trips = pd.read_csv(\"data/Household_Travel_Survey_Trips.csv\")\n",
    "persons = pd.read_csv(\"data/Household_Travel_Survey_Persons.csv\")\n",
    "households = pd.read_csv(\"data/Household_Travel_Survey_Households.csv\")\n",
    "\n",
    "# download this from github-- PRIVATE\n",
    "od = pd.read_csv(\"data/od_data/trips_2017_2019_2021_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what information do we have?\n",
    "for col in persons.columns: print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Clean data\n",
    "\n",
    "#### What do we need to clean?\n",
    "- Outliers (travel time, spatial)\n",
    "- Research-question specific (weekend data, specific trip purposes, etc.)\n",
    "- Missing/no repsonse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check weekend data. codebook say 1 is Monday, 7 is Sunday.\n",
    "trips.daynum.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our weekend trips are underrepresented, only accounting for ~18% of the data (assuming people make equal trips each day, ~28.6% would be representative).\n",
    "\n",
    "Let's see how travel time looks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(trips.travel_time)\n",
    "plt.xlim(0, 200)\n",
    "plt.xlabel(\"Travel Time (minutes)\")\n",
    "plt.title(f\"Distribution of Travel Times (n={len(trips)})\")\n",
    "plt.axvline(trips.travel_time.mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# note: this is UNWEIGHTED and should not be interpreted for region-wide analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.travel_time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a pretty long tail of travel times. Also notice some respondent bias-- people tend to think of their travel time in intervals of five. \n",
    "\n",
    "Since a lot of accessibility/mobility metrics can be derived from travel time, let's check the quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "trips.travel_time.isna().value_counts() # uh oh, 63,000 missing values?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are we missing as many datetime strings?\n",
    "print(trips.arrival_time_string.isna().value_counts(), '\\n') # looks a lot better, only 6 missing\n",
    "print(trips.depart_time_string.isna().value_counts()) # only 20 missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the missing datetime strings\n",
    "trips_clean = trips.dropna(subset=['depart_time_string', 'arrival_time_string']) # this only drops 23 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's manually calculate them using the start and end times. this way we can preserve many more observations.\n",
    "\n",
    "def travel_time_manual(row):\n",
    "    ''' \n",
    "    function to calculate travel time from datetime arrival/depart strings.\n",
    "    input: row of dataframe\n",
    "    output: travel time in minutes\n",
    "    '''\n",
    "    date_str_arr = row.arrival_time_string[:-1]\n",
    "    date_str_dep = row.depart_time_string[:-1]\n",
    "    date_format = \"Date: %Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "    # convert string to datetime object\n",
    "    dt_object_arr = datetime.strptime(date_str_arr, date_format)\n",
    "    dt_object_dep = datetime.strptime(date_str_dep, date_format)\n",
    "    \n",
    "    # calculate travel time\n",
    "    delta_time = abs(dt_object_arr - dt_object_dep)\n",
    "    travel_time_mins = delta_time.total_seconds()/60\n",
    "\n",
    "    return travel_time_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_clean.loc[:, \"travel_time_calc\"] = trips_clean.apply(travel_time_manual, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_clean.travel_time_calc.isna().value_counts() # no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create a final, cleaned dataset, let's say we want to look at just workday trips. We can assume that a \"normal\" trip is under 2 hours (adjust this for your purposes) since people need to go to schoo/work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset\n",
    "trips_clean = trips_clean[(trips_clean.travel_time_calc < 120) & (trips_clean.daynum.isin([1,2,3,4,5]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much is retained?\n",
    "print(f\"{len(trips_clean)/len(trips)*100:.2f}% of the original dataset is retained after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Summary Statistics\n",
    "\n",
    "Ok, so the above give us an idea of the *sample* behaviors, but how can we generalize this to the entire Puget Sound region?\n",
    "- Survey weights!\n",
    "    - Generally, apply weights to each year. See codebook for more detail.\n",
    "    - Note: survey weights are used so that the sampled individuals match ACS data, and that trip modes/purposes are representative. Read more here: https://www.psrc.org/media/3631\n",
    "\n",
    "Using survey weights, we can make accurate comparisons between years when describing our data. Weights are needed for things such as visualizations and summary statistics, but not for models (since in models we want to understand relationships, which does not require representativeness).\n",
    "\n",
    "\n",
    "\n",
    "Here's a cheatsheet for weighting:\n",
    "| Summary level | 2017 | 2019 | 2017/2019 | 2021\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Household | hh_weight_2017_v2021 | hh_weight_2019_v2021 | hh_weight_2017_2019_v2021 | hh_weight_2021_ABS |\n",
    "| Person | hh_weight_2017* | hh_weight_2019* | hh_weight_2017_2019* | person_adult_weight_2021 |\n",
    "| Trip | trip_weight_2017_v2021* | trip_weight_2019_v2021* | trip_weight_2017_2019_v2021* | trip_weight_2021_ABS_Panel_adult |\n",
    "| Respondent | | | | Apply to articular questions** |\n",
    "\n",
    "Notice there are some adult weights too-- 2021 sample does not contain children, while 2017/2019 does. So to compare trips between 2017/2019 and 2021, need to filter to only adult trips before applying weights. DO NOT COMPARE TRIPS WEIGHTED WITH CHILDREN TO WEIGHTS THAT DO NOT CONSIDER CHILDREN.\n",
    "\n",
    "*: filter to adults only first to compare to 2021\n",
    "\n",
    "**: respondent weights only apply to 2021 sample for\n",
    "- workplace_pre_covid\n",
    "- commute_freq_pre_covid\n",
    "- commute_mode_pre_covid\n",
    "- telecommute_freq_pre_covid\n",
    "- employment_change_employer\n",
    "- employment_change_location\n",
    "- employment_change_new_job\n",
    "- employment_change_laid_off\n",
    "- employment_change_left_workforce\n",
    "- employment_change_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that person, household, and trip dfs all have survey weights\n",
    "trips_clean.iloc[:, -8:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, how do we apply weights?\n",
    "# visually\n",
    "sns.kdeplot(trips_clean[trips_clean.survey_year ==2017].travel_time_calc, label=\"Unweighted\")\n",
    "sns.kdeplot(trips_clean[trips_clean.survey_year ==2017].travel_time_calc, weights=trips_clean.trip_weight_2017_v2021, label=\"Weighted\")\n",
    "plt.xlim(-10, 120)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Travel Times in 2017, Weight Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, with weights, we can compare trips between years\n",
    "\n",
    "# filter 2017/2019 to be adult only\n",
    "trips_clean_adults = trips_clean[(trips_clean.age != \"Under 5 years old\") | (trips_clean.age != \"5-11 years\") | \n",
    "                                 (trips_clean.age != \"12-15 years\") | (trips_clean.age != \"16-17 years\")]\n",
    "\n",
    "# now we can compare weighted trips across years\n",
    "sns.kdeplot(trips_clean_adults[trips_clean_adults.survey_year ==2017].travel_time_calc, weights=trips_clean_adults.trip_weight_2017_v2021, label=\"2017\")\n",
    "sns.kdeplot(trips_clean_adults[trips_clean_adults.survey_year ==2019].travel_time_calc, weights=trips_clean_adults.trip_weight_2019_v2021, label=\"2019\")\n",
    "sns.kdeplot(trips_clean_adults[trips_clean_adults.survey_year ==2021].travel_time_calc, weights=trips_clean_adults.trip_weight_2021_ABS_Panel_adul, label=\"2021\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted descriptive statistics\n",
    "trips_clean_2017 = trips_clean[trips_clean.survey_year == 2017]\n",
    "\n",
    "# from statsmodels\n",
    "ws_tt = DescrStatsW(trips_clean_2017.travel_time_calc, weights=trips_clean_2017.trip_weight_2017_v2021)\n",
    "quantiles = ws_tt.quantile(probs=np.array([0.25,0.5,0.75]), return_pandas=False)\n",
    "desc_stats = {\"mean\": ws_tt.mean, \"std\": ws_tt.std, \"25th percentile\": quantiles[0],\n",
    "              \"50th percentile\": quantiles[1], \"75th percentile\": quantiles[2],\n",
    "              \"min\": trips_clean_2017.travel_time_calc.min(), \"max\": trips_clean_2017.travel_time_calc.max()}\n",
    "desc_stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Mobility metrics\n",
    "Let's calculate some metrics on the daily, person level\n",
    "- number of trips\n",
    "- VMT\n",
    "- person miles traveled\n",
    "- trip chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need data for each person each day\n",
    "person_day_df = trips_clean.groupby(['person_dim_id', 'daynum']).size().reset_index()[['person_dim_id', 'daynum']]\n",
    "person_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_metrics(row):\n",
    "    ''' \n",
    "    function that calculates mobility metrics\n",
    "    input: df row\n",
    "    output: pandas series of metrics\n",
    "    '''\n",
    "    # filter to obtain trips that correspond with each person\n",
    "    trips_day_person = trips_clean[(trips_clean.person_dim_id == row.person_dim_id) & (trips_clean.daynum == row.daynum)]\n",
    "\n",
    "    # number of trips\n",
    "    num_trips = len(trips_day_person)\n",
    "\n",
    "    # person miles traveled\n",
    "    pmt = trips_day_person.trip_path_distance.sum()\n",
    "\n",
    "    # vehicle miles traveled\n",
    "    vmt = trips_day_person.loc[trips_day_person['mode_simple'] == 'Drive', 'trip_path_distance'].sum()\n",
    "\n",
    "    # trip chains\n",
    "    trip_chains = 0\n",
    "    start_home = False\n",
    "    for idx, row in trips_day_person.iterrows():\n",
    "        if row.origin_purpose == \"Went home\":\n",
    "            start_home = True\n",
    "        if (row.dest_purpose != \"Went home\") and start_home: # person continues their trip chain\n",
    "            continue\n",
    "        elif (row.dest_purpose == \"Went home\") and (row.origin_purpose == \"Went home\"): # home-home trip, not counted as a trip chain\n",
    "            continue\n",
    "        elif (row.dest_purpose == \"Went home\") and start_home: # end of the trip chain\n",
    "            trip_chains += 1\n",
    "            start_home = False # reset flag\n",
    "    \n",
    "    return pd.Series([num_trips, pmt, vmt, trip_chains])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_day_df[['num_trips', 'pmt', 'vmt', 'trip_chains']] = person_day_df.apply(mobility_metrics, axis=1)\n",
    "person_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize VMT-- let's not use weights here since it gets a little tricky since\n",
    "# we're at both the person and trip level\n",
    "plt.hist(person_day_df.vmt)\n",
    "plt.show()\n",
    "\n",
    "# still looks like there's an outlier...let's drop it\n",
    "person_day_df_clean = person_day_df[person_day_df.vmt < 500]\n",
    "plt.hist(person_day_df_clean.vmt, bins=100)\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel(\"Vehicle Miles Traveled\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about vmt by gender?\n",
    "# merge person data with person_day_df\n",
    "person_day_df_clean = person_day_df_clean.merge(persons[[\"person_id\", \"gender\"]], left_on='person_dim_id', \n",
    "                                                right_on=\"person_id\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop some outliers\n",
    "person_day_df_clean = person_day_df_clean[person_day_df_clean.vmt < 100]\n",
    "\n",
    "# let's split up by gender\n",
    "person_day_df_clean_fem = person_day_df_clean[person_day_df_clean[\"gender\"] == \"Female\"]\n",
    "person_day_df_clean_nonfem = person_day_df_clean[person_day_df_clean[\"gender\"] != \"Female\"]\n",
    "\n",
    "sns.boxplot(data=[person_day_df_clean_fem.vmt, person_day_df_clean_nonfem.vmt])\n",
    "plt.xticks(ticks=[0,1], labels=[\"Female\", \"Non-female\"])\n",
    "\n",
    "print(\"Female mean VMT:\", person_day_df_clean_fem.vmt.mean(), \"\\nMale mean VMT:\", person_day_df_clean_nonfem.vmt.mean())\n",
    "# can perform t-test to determine significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: O/D data + mapping\n",
    "We can also use O/D data collected from the online survey to calculate some more spatial metrics, such as radius of gyration (activity space).\n",
    "\n",
    "However, we need to clean this data too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out trips outside WA\n",
    "def are_points_inside_washington(origin_lat, origin_lng, dest_lat, dest_lng):\n",
    "    washington_bbox = {\n",
    "    'lat_min': 45.5435,\n",
    "    'lat_max': 49.0025,\n",
    "    'lon_min': -124.848974,\n",
    "    'lon_max': -116.916197\n",
    "    }\n",
    "    return (washington_bbox['lat_min'] <= origin_lat <= washington_bbox['lat_max']) and \\\n",
    "           (washington_bbox['lon_min'] <= origin_lng <= washington_bbox['lon_max']) and \\\n",
    "           (washington_bbox['lat_min'] <= dest_lat <= washington_bbox['lat_max']) and \\\n",
    "           (washington_bbox['lon_min'] <= dest_lng <= washington_bbox['lon_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gdf for O/D trips\n",
    "gdf_od = gpd.GeoDataFrame(od, geometry=gpd.points_from_xy(od['origin_lng'], od['origin_lat']), crs={'init':'EPSG:4326'})\n",
    "\n",
    "# get rid of nas\n",
    "gdf_od = gdf_od.dropna(subset=['origin_lng', 'origin_lat', 'dest_lng', 'dest_lat'])\n",
    "\n",
    "# limit data to washington state\n",
    "gdf_od['Both_Points_Inside_Washington'] = gdf_od.apply(lambda row: are_points_inside_washington(row['origin_lat'],\n",
    "                                                                                         row['origin_lng'],\n",
    "                                                                                         row['dest_lat'],\n",
    "                                                                                         row['dest_lng']), axis=1)\n",
    "gdf_od = gdf_od[gdf_od.Both_Points_Inside_Washington == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in washington state counties\n",
    "counties_url = \"https://gisdata.kingcounty.gov/arcgis/rest/services/OpenDataPortal/politicl___base/MapServer/122/query?outFields=*&where=1%3D1&f=geojson\"\n",
    "response = requests.get(counties_url)\n",
    "data = response.json()\n",
    "gdf_counties = gpd.GeoDataFrame.from_features(data['features'], crs={'init':'EPSG:4326'})\n",
    "\n",
    "# adjust crs\n",
    "gdf_counties = gdf_counties.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "gdf_counties.plot(ax=ax)\n",
    "gdf_od.plot(ax=ax, color='red', markersize=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate radius of gyration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge trip and o/d data\n",
    "trips_od = pd.merge(trips_clean, od, on=[\"trip_id\", \"household_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rog(points, centroid):\n",
    "    '''\n",
    "    this function is calculates rog given unique points\n",
    "    points: list of points; centroid: (x,y)\n",
    "    returns radius of gyration\n",
    "    '''\n",
    "    d_i = [geopy.distance.geodesic(points[p], (centroid[1], centroid[0])).miles for p in range(len(points))]\n",
    "    n = len(d_i)\n",
    "    sum_di = 0\n",
    "    for i in range(n):\n",
    "        sum_di += d_i[i]**2\n",
    "    rog = math.sqrt(sum_di/n)\n",
    "     \n",
    "    return rog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rog_centroid_df(row):\n",
    "    '''\n",
    "    this function calculates the radius of gyration (ROG) for each person, using UNIQUE points\n",
    "    person_od_df: df with all trips made by a single individual\n",
    "    return rog_centroid, rog_home: ROG based around centroid, ROG based around home \n",
    "    '''\n",
    "    # filter df by personid\n",
    "    person_trips_df = trips_od[trips_od.person_dim_id == row.person_id]\n",
    "\n",
    "    # get uique points\n",
    "    unique_lat = set()\n",
    "    unique_lng = set()\n",
    "    unique_pts = set()\n",
    "    for idx, row in person_trips_df.iterrows():\n",
    "        olat = round(row.origin_lat, 3)\n",
    "        olng = round(row.origin_lng, 3)\n",
    "        dlat = round(row.dest_lat, 3)\n",
    "        dlng = round(row.dest_lng, 3)\n",
    "        unique_lat.add(olat)\n",
    "        unique_lat.add(dlat)\n",
    "        unique_lng.add(olng)\n",
    "        unique_lng.add(dlng)\n",
    "        unique_pts.add((olat, olng))\n",
    "        unique_pts.add((dlat, dlng))\n",
    "    # calculate centroid lat/lon\n",
    "    centroid = (np.mean(list(unique_lng)), np.mean(list(unique_lat)))\n",
    "    \n",
    "    try:\n",
    "        # calculate ROG around centroid\n",
    "        rog_centroid = rog(list(unique_pts), centroid)\n",
    "        return rog_centroid\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons[\"rog\"] = persons.apply(rog_centroid_df, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace data is also available for 2017 data (and for 2019, though we do not have that data from PSRC). See more examples of trace data with Minda & Jaime's work from the summer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
